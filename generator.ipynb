{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a54ca62",
   "metadata": {},
   "source": [
    "!pip install ipykernel datasets pytorch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 accelerate torch==2.6.0+cu124 tensorflow tf-keras\n",
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eee9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3f4005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61aa660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "# Add special tokens\n",
    "special_tokens_dict = {\n",
    "    'pad_token': '<|pad|>',\n",
    "    'bos_token': '<|startofpoem|>',\n",
    "    'eos_token': '<|endofpoem|>'\n",
    "}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "tokenizer.pad_token = '<|pad|>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455b752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mport pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load structured data (e.g., from CSV)\n",
    "df = pd.read_csv(\"PoetryFoundationData.csv\")\n",
    "\n",
    "# Drop short poems or empty entries\n",
    "df = df[df[\"Poem\"].str.len() > 50]\n",
    "df.fillna(\"\", inplace=True)\n",
    "\n",
    "# Format input text clearly with special tokens\n",
    "def format_row(row):\n",
    "    return (\n",
    "        f\"<|startofpoem|>\\n\"\n",
    "        f\"Title: {row['Title'].strip()}\\n\"\n",
    "        f\"Poet: {row['Poet'].strip()}\\n\"\n",
    "        f\"Tags: {row['Tags'].strip()}\\n\\n\"\n",
    "        f\"{row['Poem'].strip()}\\n\"\n",
    "        f\"<|endofpoem|>\"\n",
    "    )\n",
    "\n",
    "df[\"text\"] = df.apply(format_row, axis=1)\n",
    "dataset = Dataset.from_dict({\"text\": df[\"text\"].tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a269ffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=768,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca50c9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "train_size = int(0.8 * len(tokenized_dataset))\n",
    "train_dataset = tokenized_dataset.select(range(train_size))\n",
    "valid_dataset = tokenized_dataset.select(range(train_size, len(tokenized_dataset)))\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': valid_dataset\n",
    "})\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7393ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# 1. Resize embeddings for added tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 2. Training arguments with tweaks for better creativity & training stability\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,                      # More epochs for better learning\n",
    "    per_device_train_batch_size=4,           # Larger batch size if GPU allows\n",
    "    gradient_accumulation_steps=2,           # Effective batch size = 8\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    warmup_steps=200,                        # More warmup for smoother LR ramp-up\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=3e-5,                      # Slightly lower LR for stability\n",
    "    metric_for_best_model=\"loss\",            # Use loss to pick best model\n",
    "    greater_is_better=False,\n",
    "    \n",
    ")\n",
    "\n",
    "# 3. Enable gradient checkpointing to save memory during training\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# 4. Optional: Compute perplexity for evaluation (better than empty function)\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Shift labels to align with predictions\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    \n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    perplexity = math.exp(loss.item())\n",
    "    return {\"loss\": loss.item(), \"perplexity\": perplexity}\n",
    "\n",
    "# 5. Initialize Trainer with all components\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset_dict['train'],\n",
    "    eval_dataset=dataset_dict['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 6. Start training\n",
    "trainer.train()\n",
    "\n",
    "# 7. Save final model and tokenizer\n",
    "model.save_pretrained(\"./fine_tuned_poetry_model_v3\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_poetry_model_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3589f67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Define scoring function\n",
    "def score_poem(poem_text, model, tokenizer):\n",
    "    poem_text = f\"<|startofpoem|>\\n{poem_text}\\n<|endofpoem|>\"\n",
    "    \n",
    "    inputs = tokenizer(poem_text, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    \n",
    "    return {\n",
    "        \"loss\": loss.item(),\n",
    "        \"perplexity\": perplexity.item()\n",
    "    }\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "model_path = './fine_tuned_poetry_model_v3'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Set special tokens\n",
    "special_tokens_dict = {\n",
    "    'pad_token': '<|pad|>',\n",
    "    'bos_token': '<|startofpoem|>',\n",
    "    'eos_token': '<|endofpoem|>'\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "tokenizer.pad_token = '<|pad|>'\n",
    "\n",
    "# Create output directory\n",
    "output_dir = os.path.join(model_path, 'generated_poems')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Prompt info\n",
    "title = \"Nature and Humanity\"\n",
    "poet = \"myself\"\n",
    "tags = \"Peace, Hope, Joy, Struggle, Victory, Unity, Diversity\"\n",
    "\n",
    "prompt = (\n",
    "    f\"<|startofpoem|>\\n\"\n",
    "    f\"Title: {title}\\n\"\n",
    "    f\"Poet: {poet}\\n\"\n",
    "    f\"Tags: {tags}\\n\\n\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate poems\n",
    "outputs = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    max_length=150,\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    top_k=100,\n",
    "    repetition_penalty=1.5,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=50,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.convert_tokens_to_ids(\"<|endofpoem|>\"),\n",
    ")\n",
    "\n",
    "# Save poems to CSV\n",
    "csv_path = os.path.join(output_dir, 'generated_poems.csv')\n",
    "average_perplexity = 0.0\n",
    "\n",
    "with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Poem Number', 'Title', 'Poet', 'Tags', 'Poem', 'Loss', 'Perplexity'])\n",
    "\n",
    "    for i, output in enumerate(outputs):\n",
    "        poem = tokenizer.decode(output, skip_special_tokens=True).strip()\n",
    "        score = score_poem(poem, model, tokenizer)\n",
    "        average_perplexity += score['perplexity']\n",
    "\n",
    "        writer.writerow([\n",
    "            f\"Poem #{i+1}\",\n",
    "            title,\n",
    "            poet,\n",
    "            tags,\n",
    "            poem.replace('\\n', '\\\\n'),  # Escape newlines for CSV\n",
    "            f\"{score['loss']:.4f}\",\n",
    "            f\"{score['perplexity']:.4f}\"\n",
    "        ])\n",
    "\n",
    "        # Print to console\n",
    "        print(f\"Poem #{i+1}:\\n{poem}\\n\")\n",
    "        print(f\"Score: Loss = {score['loss']:.4f}, Perplexity = {score['perplexity']:.4f}\\n\")\n",
    "        print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "# Final average perplexity\n",
    "average_perplexity /= len(outputs)\n",
    "print(f\"Average Perplexity: {average_perplexity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
