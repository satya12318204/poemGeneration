# poemGeneration
 Poem Generator using GPT-2 Overview This project fine-tunes OpenAI's GPT-2 model to generate poems using structured metadata such as Title, Poet, and Tags. It allows for creative control and scores the generated poems using loss and perplexity. Technologies Used - Python 3.x - Hugging Face Transformers - PyTorch with CUDA - TensorFlow (for tf-keras tokenizer compatibility) - Hugging Face Datasets - Pandas, CSV, NumPy Model Overview Base model: openai-community/gpt2 Special tokens added:   - <|startofpoem|>: Beginning of a poem   - <|endofpoem|>: End of a poem   - <|pad|>: Padding  Prompt Format: <|startofpoem|> Title: Nature and Humanity Poet: myself Tags: Peace, Hope, Joy, Struggle, Victory, Unity, Diversity  <poem body> <|endofpoem|> Project Structure generator.ipynb              - Jupyter Notebook with full pipeline PoetryFoundationData.csv     - Poem dataset fine_tuned_poetry_model_v3/  - Directory with model & tokenizer generated_poems.csv          - 50 generated poems with metrics README.md                    - Documentation Setup Instructions Clone and install:   git clone https://github.com/satya12318204/poem-generator.git   cd poem-generator   pip install -r requirements.txt  Install required packages:   pip install ipykernel datasets torch torchvision torchaudio accelerate tensorflow tf-keras   pip install -U transformers How to Train 1. Preprocess poems from PoetryFoundationData.csv 2. Format each poem with special tokens 3. Tokenize with GPT-2 tokenizer 4. Fine-tune GPT-2 using Trainer API:    - batch_size=4, gradient_accumulation=2    - num_train_epochs=10    - learning_rate=3e-5, fp16 if CUDA available 5. Save the model and tokenizer Generation & Scoring Generates 50 poems using top-k and top-p sampling. Each poem is scored using CrossEntropy loss and Perplexity. Saved to generated_poems.csv. Evaluation Metric - Loss: CrossEntropy - Perplexity: exp(loss), lower is better Exported Model Usage from transformers import GPT2LMHeadModel, GPT2Tokenizer model = GPT2LMHeadModel.from_pretrained('./fine_tuned_poetry_model_v3') tokenizer = GPT2Tokenizer.from_pretrained('./fine_tuned_poetry_model_v3') License MIT License
